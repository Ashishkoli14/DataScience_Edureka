{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8hLoZOM_yBQ"
   },
   "source": [
    "## Day 46 - DIY Solution\n",
    "**Q1. Problem Statement: Feature Extraction** <br>\n",
    "Write a Python program that reads the demotext2.txt text file (provided on LMS).The following are the tasks that are to be taken into consideration while\n",
    "constructing the solution.\n",
    "1. Load the demotext2.txt text file into a variable and then close the file\n",
    "2. Do sentence wise tokenization and list out generated tokens\n",
    "3. Transform each token into a lower case\n",
    "4. Do vectorization using TFID Vectorizer\n",
    "5. Generate vector-matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_7sIDumg_yBU",
    "outputId": "74d6bdbd-6faa-4824-9366-8d040060f824"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4e74rw4_yBV"
   },
   "source": [
    "**Step-1:** Import Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iRUoWZkI_yBV"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cw8TAWeB_yBW"
   },
   "source": [
    "**Step-2:** Loading the given text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sInot144_yBX",
    "outputId": "dc58f2da-9da7-4113-c92f-4373e86a4bed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He can even spout some sports trivia and Christmas carols and stuff like that.\n",
      "We'd talk sports and stuff, and maybe have a beer.\n",
      "The Admirable Crichton of his day, he was keen alike on field sports and the arts, the friend and admirer equally of Cecil Rhodes and of Rodin, a railway director and a yeomanry colonel.\n",
      "But he was not brought forward by his father or prepared in any way for his future greatness, and lived in the country occupied with field sports, till after the institution of the second protectorate in 16J7 and the recognition of Oliver's right to name his successor.\n",
      "Thankfully, he was too concerned with sports to get in any real trouble.\n",
      "See Strutt, Sports and Pastimes, who also gives an illustration, \"taken from a manuscriptal painting of the 9th century in the Cotton Library,\" representing \"a Saxon chieftain, attended by his huntsman and a couple of hounds, pursuing the wild swine in a forest.\"\n",
      "As they entered the yard, Carmen noticed Lori's little red sports car.\n",
      "After a lengthy shower, Jenn exited and pulled on clean leggings, sports bra, and socks.\n",
      "A park and sports ground at the western end of the town contains the pedestal for a statue of President Kruger.\n",
      "There were crude medieval notions that fossils were \" freaks \" or \" sports \" of nature (lusus naturae), or that they represented failures of a creative force within the earth (a notion of Greek and Arabic origin), or that larger and smaller fossils represented the remains of races of giants or of pygmies (the mythical idea).\n"
     ]
    }
   ],
   "source": [
    "f = open(\"demotext2.txt\", \"r\")\n",
    "#read whole file to a string\n",
    "text = f.read()\n",
    " \n",
    "#close file\n",
    "f.close()\n",
    " \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXZbgh_S_yBY"
   },
   "source": [
    "**Step-3:** Tokenizing the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyJNEthO_yBY",
    "outputId": "5fe326b8-74d3-4845-dfb5-113d9ed78e85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He can even spout some sports trivia and Christmas carols and stuff like that.', \"We'd talk sports and stuff, and maybe have a beer.\", 'The Admirable Crichton of his day, he was keen alike on field sports and the arts, the friend and admirer equally of Cecil Rhodes and of Rodin, a railway director and a yeomanry colonel.', \"But he was not brought forward by his father or prepared in any way for his future greatness, and lived in the country occupied with field sports, till after the institution of the second protectorate in 16J7 and the recognition of Oliver's right to name his successor.\", 'Thankfully, he was too concerned with sports to get in any real trouble.', 'See Strutt, Sports and Pastimes, who also gives an illustration, \"taken from a manuscriptal painting of the 9th century in the Cotton Library,\" representing \"a Saxon chieftain, attended by his huntsman and a couple of hounds, pursuing the wild swine in a forest.\"', \"As they entered the yard, Carmen noticed Lori's little red sports car.\", 'After a lengthy shower, Jenn exited and pulled on clean leggings, sports bra, and socks.', 'A park and sports ground at the western end of the town contains the pedestal for a statue of President Kruger.', 'There were crude medieval notions that fossils were \" freaks \" or \" sports \" of nature (lusus naturae), or that they represented failures of a creative force within the earth (a notion of Greek and Arabic origin), or that larger and smaller fossils represented the remains of races of giants or of pygmies (the mythical idea).']\n"
     ]
    }
   ],
   "source": [
    "nltk_tokens = nltk.sent_tokenize(text)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neo09T_Z_yBZ"
   },
   "source": [
    "**Step-4:** Converting  all the words in the sentences to lower case for generating tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "smSEyskd_yBZ",
    "outputId": "af291e93-b2f3-44d0-b579-d1476a3dad47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the admirable crichton of his day, he was keen alike on field sports and the arts, the friend and admirer equally of cecil rhodes and of rodin, a railway director and a yeomanry colonel.',\n",
       " 'see strutt, sports and pastimes, who also gives an illustration, \"taken from a manuscriptal painting of the 9th century in the cotton library,\" representing \"a saxon chieftain, attended by his huntsman and a couple of hounds, pursuing the wild swine in a forest.\"',\n",
       " 'a park and sports ground at the western end of the town contains the pedestal for a statue of president kruger.',\n",
       " 'thankfully, he was too concerned with sports to get in any real trouble.',\n",
       " \"as they entered the yard, carmen noticed lori's little red sports car.\",\n",
       " 'after a lengthy shower, jenn exited and pulled on clean leggings, sports bra, and socks.',\n",
       " \"we'd talk sports and stuff, and maybe have a beer.\",\n",
       " 'he can even spout some sports trivia and christmas carols and stuff like that.',\n",
       " \"but he was not brought forward by his father or prepared in any way for his future greatness, and lived in the country occupied with field sports, till after the institution of the second protectorate in 16j7 and the recognition of oliver's right to name his successor.\",\n",
       " 'there were crude medieval notions that fossils were \" freaks \" or \" sports \" of nature (lusus naturae), or that they represented failures of a creative force within the earth (a notion of greek and arabic origin), or that larger and smaller fossils represented the remains of races of giants or of pygmies (the mythical idea).']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Setting all the sentance  to lower case\n",
    "nltk_tokens = set(w.lower() for w in nltk_tokens)\n",
    "nltk_tokens = list(nltk_tokens)\n",
    "nltk_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajKBK2_B_yBa"
   },
   "source": [
    "**Step-5:** Vectorizing sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RvOG2eWl_yBa",
    "outputId": "73fcc3a6-982b-4751-f703-065a01fa2b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 19)\t0.24354521874367308\n",
      "  (0, 123)\t0.24354521874367308\n",
      "  (0, 29)\t0.24354521874367308\n",
      "  (0, 90)\t0.24354521874367308\n",
      "  (0, 99)\t0.24354521874367308\n",
      "  (0, 97)\t0.24354521874367308\n",
      "  (0, 14)\t0.24354521874367308\n",
      "  (0, 33)\t0.24354521874367308\n",
      "  (0, 3)\t0.24354521874367308\n",
      "  (0, 43)\t0.24354521874367308\n",
      "  (0, 6)\t0.24354521874367308\n",
      "  (0, 105)\t0.09004358647762914\n",
      "  (0, 37)\t0.2070356862180743\n",
      "  (0, 4)\t0.24354521874367308\n",
      "  (0, 56)\t0.24354521874367308\n",
      "  (0, 28)\t0.24354521874367308\n",
      "  (0, 26)\t0.24354521874367308\n",
      "  (0, 2)\t0.24354521874367308\n",
      "  (1, 39)\t0.21254144490425056\n",
      "  (1, 111)\t0.21254144490425056\n",
      "  (1, 121)\t0.21254144490425056\n",
      "  (1, 87)\t0.21254144490425056\n",
      "  (1, 50)\t0.21254144490425056\n",
      "  (1, 24)\t0.21254144490425056\n",
      "  (1, 51)\t0.21254144490425056\n",
      "  :\t:\n",
      "  (9, 70)\t0.17921062626016357\n",
      "  (9, 88)\t0.17921062626016357\n",
      "  (9, 45)\t0.17921062626016357\n",
      "  (9, 89)\t0.17921062626016357\n",
      "  (9, 94)\t0.17921062626016357\n",
      "  (9, 103)\t0.17921062626016357\n",
      "  (9, 58)\t0.17921062626016357\n",
      "  (9, 78)\t0.17921062626016357\n",
      "  (9, 5)\t0.17921062626016357\n",
      "  (9, 48)\t0.17921062626016357\n",
      "  (9, 74)\t0.17921062626016357\n",
      "  (9, 30)\t0.17921062626016357\n",
      "  (9, 38)\t0.17921062626016357\n",
      "  (9, 25)\t0.17921062626016357\n",
      "  (9, 35)\t0.17921062626016357\n",
      "  (9, 95)\t0.35842125252032714\n",
      "  (9, 71)\t0.17921062626016357\n",
      "  (9, 66)\t0.17921062626016357\n",
      "  (9, 72)\t0.17921062626016357\n",
      "  (9, 42)\t0.17921062626016357\n",
      "  (9, 41)\t0.35842125252032714\n",
      "  (9, 75)\t0.17921062626016357\n",
      "  (9, 69)\t0.17921062626016357\n",
      "  (9, 27)\t0.17921062626016357\n",
      "  (9, 105)\t0.06625778821119362\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_vect = TfidfVectorizer(min_df=1,lowercase=True,stop_words='english')\n",
    "tf_matrix = tf_vect.fit_transform(nltk_tokens)\n",
    "print(tf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzML6nUG_yBb"
   },
   "source": [
    "**Step-7:** Vector matrix formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "geVLAx91_yBb",
    "outputId": "aa43a856-d0ec-4b9e-d9c1-b46cba2a6117"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>16j7</th>\n",
       "      <th>9th</th>\n",
       "      <th>admirable</th>\n",
       "      <th>admirer</th>\n",
       "      <th>alike</th>\n",
       "      <th>arabic</th>\n",
       "      <th>arts</th>\n",
       "      <th>attended</th>\n",
       "      <th>beer</th>\n",
       "      <th>bra</th>\n",
       "      <th>...</th>\n",
       "      <th>thankfully</th>\n",
       "      <th>till</th>\n",
       "      <th>town</th>\n",
       "      <th>trivia</th>\n",
       "      <th>trouble</th>\n",
       "      <th>way</th>\n",
       "      <th>western</th>\n",
       "      <th>wild</th>\n",
       "      <th>yard</th>\n",
       "      <th>yeomanry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.243545</td>\n",
       "      <td>0.243545</td>\n",
       "      <td>0.243545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.243545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.243545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212541</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.49167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.49167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.33083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50903</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.413119</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.224397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.224397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.224397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       16j7       9th  admirable   admirer     alike    arabic      arts  \\\n",
       "0  0.000000  0.000000   0.243545  0.243545  0.243545  0.000000  0.243545   \n",
       "1  0.000000  0.212541   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8  0.224397  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9  0.000000  0.000000   0.000000  0.000000  0.000000  0.179211  0.000000   \n",
       "\n",
       "   attended     beer      bra  ...  thankfully      till      town    trivia  \\\n",
       "0  0.000000  0.00000  0.00000  ...     0.00000  0.000000  0.000000  0.000000   \n",
       "1  0.212541  0.00000  0.00000  ...     0.00000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.00000  0.00000  ...     0.00000  0.000000  0.314088  0.000000   \n",
       "3  0.000000  0.00000  0.00000  ...     0.49167  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.00000  0.00000  ...     0.00000  0.000000  0.000000  0.000000   \n",
       "5  0.000000  0.00000  0.33083  ...     0.00000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.50903  0.00000  ...     0.00000  0.000000  0.000000  0.000000   \n",
       "7  0.000000  0.00000  0.00000  ...     0.00000  0.000000  0.000000  0.413119   \n",
       "8  0.000000  0.00000  0.00000  ...     0.00000  0.224397  0.000000  0.000000   \n",
       "9  0.000000  0.00000  0.00000  ...     0.00000  0.000000  0.000000  0.000000   \n",
       "\n",
       "   trouble       way   western      wild      yard  yeomanry  \n",
       "0  0.00000  0.000000  0.000000  0.000000  0.000000  0.243545  \n",
       "1  0.00000  0.000000  0.000000  0.212541  0.000000  0.000000  \n",
       "2  0.00000  0.000000  0.314088  0.000000  0.000000  0.000000  \n",
       "3  0.49167  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.00000  0.000000  0.000000  0.000000  0.350571  0.000000  \n",
       "5  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "6  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "7  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "8  0.00000  0.224397  0.000000  0.000000  0.000000  0.000000  \n",
       "9  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[10 rows x 124 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tf_names = tf_vect.get_feature_names_out()\n",
    "tf_df = pd.DataFrame(tf_matrix.toarray(),columns=tf_names)\n",
    "tf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       16j7       9th  admirable   admirer     alike    arabic      arts  \\\n",
      "0  0.000000  0.000000   0.243545  0.243545  0.243545  0.000000  0.243545   \n",
      "1  0.000000  0.212541   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "3  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "4  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "5  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "6  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "7  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "8  0.224397  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "9  0.000000  0.000000   0.000000  0.000000  0.000000  0.179211  0.000000   \n",
      "\n",
      "   attended     beer      bra  ...  thankfully      till      town    trivia  \\\n",
      "0  0.000000  0.00000  0.00000  ...     0.00000  0.000000  0.000000  0.000000   \n",
      "1  0.212541  0.00000  0.00000  ...     0.00000  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.00000  0.00000  ...     0.00000  0.000000  0.314088  0.000000   \n",
      "3  0.000000  0.00000  0.00000  ...     0.49167  0.000000  0.000000  0.000000   \n",
      "4  0.000000  0.00000  0.00000  ...     0.00000  0.000000  0.000000  0.000000   \n",
      "5  0.000000  0.00000  0.33083  ...     0.00000  0.000000  0.000000  0.000000   \n",
      "6  0.000000  0.50903  0.00000  ...     0.00000  0.000000  0.000000  0.000000   \n",
      "7  0.000000  0.00000  0.00000  ...     0.00000  0.000000  0.000000  0.413119   \n",
      "8  0.000000  0.00000  0.00000  ...     0.00000  0.224397  0.000000  0.000000   \n",
      "9  0.000000  0.00000  0.00000  ...     0.00000  0.000000  0.000000  0.000000   \n",
      "\n",
      "   trouble       way   western      wild      yard  yeomanry  \n",
      "0  0.00000  0.000000  0.000000  0.000000  0.000000  0.243545  \n",
      "1  0.00000  0.000000  0.000000  0.212541  0.000000  0.000000  \n",
      "2  0.00000  0.000000  0.314088  0.000000  0.000000  0.000000  \n",
      "3  0.49167  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "4  0.00000  0.000000  0.000000  0.000000  0.350571  0.000000  \n",
      "5  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "6  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "7  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "8  0.00000  0.224397  0.000000  0.000000  0.000000  0.000000  \n",
      "9  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[10 rows x 124 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming you have your text data in a list called 'nltk_tokens'\n",
    "tf_vect = TfidfVectorizer(min_df=1, lowercase=True, stop_words='english')\n",
    "tf_matrix = tf_vect.fit_transform(nltk_tokens)\n",
    "\n",
    "# Get the feature names and create a DataFrame\n",
    "tf_names = tf_vect.get_feature_names_out()\n",
    "tf_df = pd.DataFrame(tf_matrix.toarray(), columns=tf_names)\n",
    "print(tf_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaX_7mQ-mN2n"
   },
   "source": [
    "**Conclusion:** The sentences in the dataset has been changed into word tokens and then later converted to Vectors"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS_D46_DIY_Solution_V1_0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
