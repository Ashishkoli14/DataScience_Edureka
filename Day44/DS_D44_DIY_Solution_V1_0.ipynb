{"cells":[{"cell_type":"markdown","metadata":{"id":"bERgoniaJw8g"},"source":["## Day 44 - DIY Solution\n","**Q1. Problem Statement: NLP text processing**<br>\n","Write a Python program that reads the demotext.txt text file(provided on LMS).The following are the tasks that are to be taken into consideration while\n","constructing the solution for text processing using the NLTK library.\n","1.\tLoad the demotext.txt text file into a variable and then close the file\n","2.\tDo word wise tokenization list out generated tokens\n","3.\tTransform each token into a small case\n","4.\tRemove stop words from the generated token list\n","5.\tRemove extra symbols like commas, full stops, and question marks using a regular expression tokenizer and store them in another variable\n","6.\tDo bigram and trigram for generated tokens\n"]},{"cell_type":"markdown","metadata":{"id":"qFcDvSZqJw8o"},"source":["**Step-1:** Importing Libraries."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBCpEL0RJw8p"},"outputs":[],"source":["import nltk\n","from nltk import word_tokenize\n","\n","#The download function will help in downloading the corporas present in the nltk library\n","#nltk.download()\n","\n","from nltk.corpus import reuters\n","from nltk.tokenize import RegexpTokenizer"]},{"cell_type":"markdown","metadata":{"id":"s24RrTzbJw8q"},"source":["**Step-2:** Download 'punkt'."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"7Ds74YnKJw8r","outputId":"b6e24471-c72f-45e0-9a2c-778070b6e549","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650858543935,"user_tz":-330,"elapsed":767,"user":{"displayName":"Abhishek Dodiya","userId":"05457856945198063411"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["nltk.download('punkt')"]},{"cell_type":"code","source":["nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OUuP2wGC5Npf","executionInfo":{"status":"ok","timestamp":1650858543936,"user_tz":-330,"elapsed":4,"user":{"displayName":"Abhishek Dodiya","userId":"05457856945198063411"}},"outputId":"f0959c82-757f-4caa-f198-f5874dbf8ebb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"_E7ywbNZJw8t"},"source":["**Step-3:** Load given textfile into variable "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YZh7Y0PkJw8t","outputId":"b40957d1-e9f7-4f54-d65b-b90568bffb0a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650858546532,"user_tz":-330,"elapsed":3,"user":{"displayName":"Abhishek Dodiya","userId":"05457856945198063411"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["What is Lorem Ipsum?\n","\n","Lorem Ipsum is simply dummy text of the printing and \n","typesetting industry. Lorem Ipsum has been the industry's \n","standard dummy text ever since the 1500s, when an unknown \n","printer took a galley of type and scrambled it to make a \n","type specimen book. It has survived not only five centuries, \n","but also the leap into electronic typesetting, \n","remaining essentially unchanged. \n","\n","It was popularised in the 1960s with the release of Letraset \n","sheets containing Lorem Ipsum passages, and more recently \n","with desktop publishing software like Aldus PageMaker \n","including versions of Lorem Ipsum.\n"]}],"source":["f = open(\"demotext.txt\", \"r\")\n","#read whole file to a string\n","text = f.read()\n"," \n","#close file\n","f.close()\n"," \n","print(text)"]},{"cell_type":"markdown","metadata":{"id":"r5TS2w5qJw8u"},"source":["**Step-4:** Generate token using Word tokenize "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EwvEYy81Jw8w","outputId":"e96b7c8b-1302-471f-f70c-7577ee9d3c76","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650439989016,"user_tz":-330,"elapsed":456,"user":{"displayName":"Abhishek Dodiya","userId":"05457856945198063411"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['What', 'is', 'Lorem', 'Ipsum', '?', 'Lorem', 'Ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry', '.', 'Lorem', 'Ipsum', 'has', 'been', 'the', 'industry', \"'s\", 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s', ',', 'when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen', 'book', '.', 'It', 'has', 'survived', 'not', 'only', 'five', 'centuries', ',', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'typesetting', ',', 'remaining', 'essentially', 'unchanged', '.', 'It', 'was', 'popularised', 'in', 'the', '1960s', 'with', 'the', 'release', 'of', 'Letraset', 'sheets', 'containing', 'Lorem', 'Ipsum', 'passages', ',', 'and', 'more', 'recently', 'with', 'desktop', 'publishing', 'software', 'like', 'Aldus', 'PageMaker', 'including', 'versions', 'of', 'Lorem', 'Ipsum', '.']\n"]}],"source":["nltk_tokens = nltk.word_tokenize(text)\n","print (nltk_tokens)"]},{"cell_type":"markdown","metadata":{"id":"easZwySaJw8x"},"source":["**Step-5:** Convert all tokens into lower case "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vSFFU4cSJw8x","outputId":"a9c6dcd1-d7b5-4588-cde4-b0bc1aea98b1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650439995544,"user_tz":-330,"elapsed":495,"user":{"displayName":"Abhishek Dodiya","userId":"05457856945198063411"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{\"'s\",\n"," ',',\n"," '.',\n"," '1500s',\n"," '1960s',\n"," '?',\n"," 'a',\n"," 'aldus',\n"," 'also',\n"," 'an',\n"," 'and',\n"," 'been',\n"," 'book',\n"," 'but',\n"," 'centuries',\n"," 'containing',\n"," 'desktop',\n"," 'dummy',\n"," 'electronic',\n"," 'essentially',\n"," 'ever',\n"," 'five',\n"," 'galley',\n"," 'has',\n"," 'in',\n"," 'including',\n"," 'industry',\n"," 'into',\n"," 'ipsum',\n"," 'is',\n"," 'it',\n"," 'leap',\n"," 'letraset',\n"," 'like',\n"," 'lorem',\n"," 'make',\n"," 'more',\n"," 'not',\n"," 'of',\n"," 'only',\n"," 'pagemaker',\n"," 'passages',\n"," 'popularised',\n"," 'printer',\n"," 'printing',\n"," 'publishing',\n"," 'recently',\n"," 'release',\n"," 'remaining',\n"," 'scrambled',\n"," 'sheets',\n"," 'simply',\n"," 'since',\n"," 'software',\n"," 'specimen',\n"," 'standard',\n"," 'survived',\n"," 'text',\n"," 'the',\n"," 'to',\n"," 'took',\n"," 'type',\n"," 'typesetting',\n"," 'unchanged',\n"," 'unknown',\n"," 'versions',\n"," 'was',\n"," 'what',\n"," 'when',\n"," 'with'}"]},"metadata":{},"execution_count":7}],"source":["#1. Setting all the words in the file to lower case\n","nltk_tokens = set(w.lower() for w in nltk_tokens)\n","nltk_tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3oxaftzWJw8y","outputId":"d10e90e2-9f82-4b62-839a-b0c3c79eb45a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650440078634,"user_tz":-330,"elapsed":488,"user":{"displayName":"Abhishek Dodiya","userId":"05457856945198063411"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"]}],"source":["#2.example of stop words\n","# looking at some of the stopwrods in the corpus\n","from nltk.corpus import stopwords\n","stopWords = stopwords.words('english')\n","\n","# Printing the stopwords\n","print(stopWords)"]},{"cell_type":"markdown","metadata":{"id":"KmtihlPaJw8z"},"source":["**Step-6:** Remove stopwords "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L78KG4ZzJw82","outputId":"5c6243d3-77cc-48ab-935d-dce03852918d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650440083614,"user_tz":-330,"elapsed":478,"user":{"displayName":"Abhishek Dodiya","userId":"05457856945198063411"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['text',\n"," '?',\n"," 'remaining',\n"," 'dummy',\n"," '.',\n"," 'desktop',\n"," 'release',\n"," 'containing',\n"," 'ipsum',\n"," 'unknown',\n"," 'simply',\n"," 'printer',\n"," 'ever',\n"," '1960s',\n"," 'make',\n"," 'essentially',\n"," 'popularised',\n"," 'unchanged',\n"," 'five',\n"," 'versions',\n"," 'specimen',\n"," 'passages',\n"," 'sheets',\n"," 'pagemaker',\n"," 'lorem',\n"," 'leap',\n"," 'electronic',\n"," 'scrambled',\n"," 'publishing',\n"," 'centuries',\n"," 'aldus',\n"," 'software',\n"," 'printing',\n"," 'galley',\n"," 'book',\n"," ',',\n"," 'survived',\n"," 'type',\n"," 'like',\n"," '1500s',\n"," 'took',\n"," \"'s\",\n"," 'including',\n"," 'since',\n"," 'standard',\n"," 'also',\n"," 'recently',\n"," 'typesetting',\n"," 'industry',\n"," 'letraset']"]},"metadata":{},"execution_count":12}],"source":["# Importing the corpus of stopwords\n","# We can use this corpus to exclude all the stopwords from our file\n","from nltk.corpus import stopwords\n","\n","# We will form a word list by taking in the words from the file but exclude those words which are stopwords\n","wordList = [w for w in nltk_tokens if w.lower() not in stopwords.words('english')]\n","wordList"]},{"cell_type":"markdown","metadata":{"id":"UQawzGpIJw83"},"source":["**Step-7:** Remove extra symobls like comma, fullstop and question mark."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B0cdKuTJJw84","outputId":"ed0b7261-a14b-470d-c2c4-a5de7297a0c2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650440088094,"user_tz":-330,"elapsed":475,"user":{"displayName":"Abhishek Dodiya","userId":"05457856945198063411"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['What',\n"," 'is',\n"," 'Lorem',\n"," 'Ipsum',\n"," 'Lorem',\n"," 'Ipsum',\n"," 'is',\n"," 'simply',\n"," 'dummy',\n"," 'text',\n"," 'of',\n"," 'the',\n"," 'printing',\n"," 'and',\n"," 'typesetting',\n"," 'industry',\n"," 'Lorem',\n"," 'Ipsum',\n"," 'has',\n"," 'been',\n"," 'the',\n"," 'industry',\n"," 's',\n"," 'standard',\n"," 'dummy',\n"," 'text',\n"," 'ever',\n"," 'since',\n"," 'the',\n"," '1500s',\n"," 'when',\n"," 'an',\n"," 'unknown',\n"," 'printer',\n"," 'took',\n"," 'a',\n"," 'galley',\n"," 'of',\n"," 'type',\n"," 'and',\n"," 'scrambled',\n"," 'it',\n"," 'to',\n"," 'make',\n"," 'a',\n"," 'type',\n"," 'specimen',\n"," 'book',\n"," 'It',\n"," 'has',\n"," 'survived',\n"," 'not',\n"," 'only',\n"," 'five',\n"," 'centuries',\n"," 'but',\n"," 'also',\n"," 'the',\n"," 'leap',\n"," 'into',\n"," 'electronic',\n"," 'typesetting',\n"," 'remaining',\n"," 'essentially',\n"," 'unchanged',\n"," 'It',\n"," 'was',\n"," 'popularised',\n"," 'in',\n"," 'the',\n"," '1960s',\n"," 'with',\n"," 'the',\n"," 'release',\n"," 'of',\n"," 'Letraset',\n"," 'sheets',\n"," 'containing',\n"," 'Lorem',\n"," 'Ipsum',\n"," 'passages',\n"," 'and',\n"," 'more',\n"," 'recently',\n"," 'with',\n"," 'desktop',\n"," 'publishing',\n"," 'software',\n"," 'like',\n"," 'Aldus',\n"," 'PageMaker',\n"," 'including',\n"," 'versions',\n"," 'of',\n"," 'Lorem',\n"," 'Ipsum']"]},"metadata":{},"execution_count":13}],"source":["#3. remove extra symobls like comma, fullstop and questionmark.\n","tokenizer = RegexpTokenizer(r'\\w+')\n","new_token = tokenizer.tokenize(text)\n","new_token"]},{"cell_type":"markdown","metadata":{"id":"e2YnML9UJw85"},"source":["**Step-7:** generate bigram and trigram "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nUMP6l93Jw85","outputId":"94de95fb-148d-48e6-cd16-f66aeb2e0f14","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650440090961,"user_tz":-330,"elapsed":3,"user":{"displayName":"Abhishek Dodiya","userId":"05457856945198063411"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('it', 'text'),\n"," ('text', '?'),\n"," ('?', 'of'),\n"," ('of', 'been'),\n"," ('been', 'remaining'),\n"," ('remaining', 'dummy'),\n"," ('dummy', '.'),\n"," ('.', 'what'),\n"," ('what', 'desktop'),\n"," ('desktop', 'with'),\n"," ('with', 'release'),\n"," ('release', 'containing'),\n"," ('containing', 'ipsum'),\n"," ('ipsum', 'when'),\n"," ('when', 'unknown'),\n"," ('unknown', 'simply'),\n"," ('simply', 'printer'),\n"," ('printer', 'in'),\n"," ('in', 'ever'),\n"," ('ever', '1960s'),\n"," ('1960s', 'make'),\n"," ('make', 'essentially'),\n"," ('essentially', 'popularised'),\n"," ('popularised', 'unchanged'),\n"," ('unchanged', 'five'),\n"," ('five', 'versions'),\n"," ('versions', 'specimen'),\n"," ('specimen', 'passages'),\n"," ('passages', 'sheets'),\n"," ('sheets', 'pagemaker'),\n"," ('pagemaker', 'lorem'),\n"," ('lorem', 'leap'),\n"," ('leap', 'electronic'),\n"," ('electronic', 'scrambled'),\n"," ('scrambled', 'the'),\n"," ('the', 'to'),\n"," ('to', 'not'),\n"," ('not', 'into'),\n"," ('into', 'is'),\n"," ('is', 'publishing'),\n"," ('publishing', 'centuries'),\n"," ('centuries', 'aldus'),\n"," ('aldus', 'software'),\n"," ('software', 'printing'),\n"," ('printing', 'galley'),\n"," ('galley', 'book'),\n"," ('book', ','),\n"," (',', 'survived'),\n"," ('survived', 'type'),\n"," ('type', 'like'),\n"," ('like', 'and'),\n"," ('and', '1500s'),\n"," ('1500s', 'has'),\n"," ('has', 'took'),\n"," ('took', \"'s\"),\n"," (\"'s\", 'only'),\n"," ('only', 'including'),\n"," ('including', 'an'),\n"," ('an', 'since'),\n"," ('since', 'standard'),\n"," ('standard', 'also'),\n"," ('also', 'more'),\n"," ('more', 'recently'),\n"," ('recently', 'typesetting'),\n"," ('typesetting', 'industry'),\n"," ('industry', 'was'),\n"," ('was', 'letraset'),\n"," ('letraset', 'but'),\n"," ('but', 'a')]"]},"metadata":{},"execution_count":14}],"source":["##4. create bigram and trigram \n","bigrams=list(nltk.bigrams(nltk_tokens))\n","bigrams\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gr_IOrVvJw86","outputId":"0bf8a965-c236-424a-d3c7-6220be02a080","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650440093657,"user_tz":-330,"elapsed":3,"user":{"displayName":"Abhishek Dodiya","userId":"05457856945198063411"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('it', 'text', '?'),\n"," ('text', '?', 'of'),\n"," ('?', 'of', 'been'),\n"," ('of', 'been', 'remaining'),\n"," ('been', 'remaining', 'dummy'),\n"," ('remaining', 'dummy', '.'),\n"," ('dummy', '.', 'what'),\n"," ('.', 'what', 'desktop'),\n"," ('what', 'desktop', 'with'),\n"," ('desktop', 'with', 'release'),\n"," ('with', 'release', 'containing'),\n"," ('release', 'containing', 'ipsum'),\n"," ('containing', 'ipsum', 'when'),\n"," ('ipsum', 'when', 'unknown'),\n"," ('when', 'unknown', 'simply'),\n"," ('unknown', 'simply', 'printer'),\n"," ('simply', 'printer', 'in'),\n"," ('printer', 'in', 'ever'),\n"," ('in', 'ever', '1960s'),\n"," ('ever', '1960s', 'make'),\n"," ('1960s', 'make', 'essentially'),\n"," ('make', 'essentially', 'popularised'),\n"," ('essentially', 'popularised', 'unchanged'),\n"," ('popularised', 'unchanged', 'five'),\n"," ('unchanged', 'five', 'versions'),\n"," ('five', 'versions', 'specimen'),\n"," ('versions', 'specimen', 'passages'),\n"," ('specimen', 'passages', 'sheets'),\n"," ('passages', 'sheets', 'pagemaker'),\n"," ('sheets', 'pagemaker', 'lorem'),\n"," ('pagemaker', 'lorem', 'leap'),\n"," ('lorem', 'leap', 'electronic'),\n"," ('leap', 'electronic', 'scrambled'),\n"," ('electronic', 'scrambled', 'the'),\n"," ('scrambled', 'the', 'to'),\n"," ('the', 'to', 'not'),\n"," ('to', 'not', 'into'),\n"," ('not', 'into', 'is'),\n"," ('into', 'is', 'publishing'),\n"," ('is', 'publishing', 'centuries'),\n"," ('publishing', 'centuries', 'aldus'),\n"," ('centuries', 'aldus', 'software'),\n"," ('aldus', 'software', 'printing'),\n"," ('software', 'printing', 'galley'),\n"," ('printing', 'galley', 'book'),\n"," ('galley', 'book', ','),\n"," ('book', ',', 'survived'),\n"," (',', 'survived', 'type'),\n"," ('survived', 'type', 'like'),\n"," ('type', 'like', 'and'),\n"," ('like', 'and', '1500s'),\n"," ('and', '1500s', 'has'),\n"," ('1500s', 'has', 'took'),\n"," ('has', 'took', \"'s\"),\n"," ('took', \"'s\", 'only'),\n"," (\"'s\", 'only', 'including'),\n"," ('only', 'including', 'an'),\n"," ('including', 'an', 'since'),\n"," ('an', 'since', 'standard'),\n"," ('since', 'standard', 'also'),\n"," ('standard', 'also', 'more'),\n"," ('also', 'more', 'recently'),\n"," ('more', 'recently', 'typesetting'),\n"," ('recently', 'typesetting', 'industry'),\n"," ('typesetting', 'industry', 'was'),\n"," ('industry', 'was', 'letraset'),\n"," ('was', 'letraset', 'but'),\n"," ('letraset', 'but', 'a')]"]},"metadata":{},"execution_count":15}],"source":["trigrams=list(nltk.trigrams(nltk_tokens))\n","trigrams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fjO8-T6RJw86"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cJLP0yOjJw87"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"DS_D44_DIY_Solution_V1_0.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}